{"ast":null,"code":"var _jsxFileName = \"/Users/Shufan/Desktop/aipd/frontend/src/Pages/AboutPage.js\";\nimport React from 'react';\nimport { Container, Header, Grid, Image } from 'semantic-ui-react';\nimport AgentCardComponent from '../Components/AgentCardComponent';\nimport Cooperate from '../Static/Cooperate.png';\nimport Defect from '../Static/Defect.png';\nimport Copy from '../Static/Copy.png';\nimport Grudge from '../Static/Grudge.png';\nimport Model from '../Static/Model.png';\nimport LSTM from '../Static/LSTM.png';\nimport QLearning from '../Static/QLearning.png';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\n\nconst AboutPage = () => {\n  return /*#__PURE__*/_jsxDEV(Container, {\n    children: [/*#__PURE__*/_jsxDEV(Header, {\n      as: \"h2\",\n      children: \"Introduction to Prisoner's Dilemma\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 14,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      children: \"The prisoner's dilemma is a game commonly analyzed in game theory to show why individuals may not cooperate despite it appearing to be in their best interest to do so.  In this game, two players make a choice to cooperate or defect, and are rewarded based on their action and their opponent's action. The rewards are constrained such that a reward is maximum when one individual defects and the other does not. The next highest reward occurs when both individuals cooperate. Lastly, when both both individuals defect, an individual's reward is minimized. The iterated  prisoner's  dilemma  (IPD) repeats  this process multiple times, creating a greater incentive to cooperate, motivating coordination from both agents. The goal for this project was to use a combination of deep learning and reinforcement learning to create an agent that could play prisoner's dilemma to a reasonable level of proficiency.\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 15,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Header, {\n      as: \"h2\",\n      children: \"Agents for Training\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 26,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      children: \"To train our model, we needed it to play against \"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 27,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Container, {}, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 28,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Header, {\n      as: \"h2\",\n      children: \"Custom AI Agent\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 32,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      children: \"Our custom model utilized deep learning in the form of a LSTM Neural Network to predict the idenity of the agent it plays against. Then, from the identity of the agent, a custom Q-Table is used to infer the optimal move given the previous history of the game. The LSTM Neural Network\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 33,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Grid, {\n      children: /*#__PURE__*/_jsxDEV(Grid.Row, {\n        centered: true,\n        children: /*#__PURE__*/_jsxDEV(Grid.Column, {\n          width: 12,\n          children: /*#__PURE__*/_jsxDEV(Image, {\n            src: Model\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 41,\n            columnNumber: 21\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 40,\n          columnNumber: 17\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 39,\n        columnNumber: 13\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 38,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Header, {\n      as: \"h2\",\n      children: \"LSTM for Prediction\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 45,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Grid, {\n      children: /*#__PURE__*/_jsxDEV(Grid.Row, {\n        centered: true,\n        children: /*#__PURE__*/_jsxDEV(Grid.Column, {\n          width: 10,\n          children: /*#__PURE__*/_jsxDEV(Image, {\n            src: LSTM\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 49,\n            columnNumber: 21\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 48,\n          columnNumber: 17\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 47,\n        columnNumber: 13\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 46,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Header, {\n      as: \"h2\",\n      children: \"Q-Table for Action\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 53,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Grid, {\n      children: /*#__PURE__*/_jsxDEV(Grid.Row, {\n        centered: true,\n        children: /*#__PURE__*/_jsxDEV(Grid.Column, {\n          width: 12,\n          children: /*#__PURE__*/_jsxDEV(Image, {\n            src: QLearning\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 57,\n            columnNumber: 21\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 56,\n          columnNumber: 17\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 55,\n        columnNumber: 13\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 54,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Header, {\n      as: \"h2\",\n      children: \"References\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 61,\n      columnNumber: 9\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 13,\n    columnNumber: 12\n  }, this);\n};\n\n_c = AboutPage;\nexport default AboutPage;\n\nvar _c;\n\n$RefreshReg$(_c, \"AboutPage\");","map":{"version":3,"sources":["/Users/Shufan/Desktop/aipd/frontend/src/Pages/AboutPage.js"],"names":["React","Container","Header","Grid","Image","AgentCardComponent","Cooperate","Defect","Copy","Grudge","Model","LSTM","QLearning","AboutPage"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,SAASC,SAAT,EAAoBC,MAApB,EAA4BC,IAA5B,EAAkCC,KAAlC,QAA+C,mBAA/C;AACA,OAAOC,kBAAP,MAA+B,kCAA/B;AACA,OAAOC,SAAP,MAAsB,yBAAtB;AACA,OAAOC,MAAP,MAAmB,sBAAnB;AACA,OAAOC,IAAP,MAAiB,oBAAjB;AACA,OAAOC,MAAP,MAAmB,sBAAnB;AACA,OAAOC,KAAP,MAAkB,qBAAlB;AACA,OAAOC,IAAP,MAAiB,oBAAjB;AACA,OAAOC,SAAP,MAAsB,yBAAtB;;;AAEA,MAAMC,SAAS,GAAG,MAAM;AACpB,sBAAO,QAAC,SAAD;AAAA,4BACH,QAAC,MAAD;AAAQ,MAAA,EAAE,EAAC,IAAX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YADG,eAEH;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAFG,eAaH,QAAC,MAAD;AAAQ,MAAA,EAAE,EAAC,IAAX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAbG,eAcH;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAdG,eAeH,QAAC,SAAD;AAAA;AAAA;AAAA;AAAA,YAfG,eAmBH,QAAC,MAAD;AAAQ,MAAA,EAAE,EAAC,IAAX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAnBG,eAoBH;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YApBG,eAyBH,QAAC,IAAD;AAAA,6BACI,QAAC,IAAD,CAAM,GAAN;AAAU,QAAA,QAAQ,MAAlB;AAAA,+BACI,QAAC,IAAD,CAAM,MAAN;AAAa,UAAA,KAAK,EAAE,EAApB;AAAA,iCACI,QAAC,KAAD;AAAO,YAAA,GAAG,EAAEH;AAAZ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,YAzBG,eAgCH,QAAC,MAAD;AAAQ,MAAA,EAAE,EAAC,IAAX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAhCG,eAiCH,QAAC,IAAD;AAAA,6BACI,QAAC,IAAD,CAAM,GAAN;AAAU,QAAA,QAAQ,MAAlB;AAAA,+BACI,QAAC,IAAD,CAAM,MAAN;AAAa,UAAA,KAAK,EAAE,EAApB;AAAA,iCACI,QAAC,KAAD;AAAO,YAAA,GAAG,EAAEC;AAAZ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,YAjCG,eAwCH,QAAC,MAAD;AAAQ,MAAA,EAAE,EAAC,IAAX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAxCG,eAyCH,QAAC,IAAD;AAAA,6BACI,QAAC,IAAD,CAAM,GAAN;AAAU,QAAA,QAAQ,MAAlB;AAAA,+BACI,QAAC,IAAD,CAAM,MAAN;AAAa,UAAA,KAAK,EAAE,EAApB;AAAA,iCACI,QAAC,KAAD;AAAO,YAAA,GAAG,EAAEC;AAAZ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,YAzCG,eAgDH,QAAC,MAAD;AAAQ,MAAA,EAAE,EAAC,IAAX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAhDG;AAAA;AAAA;AAAA;AAAA;AAAA,UAAP;AAkDH,CAnDD;;KAAMC,S;AAqDN,eAAeA,SAAf","sourcesContent":["import React from 'react';\nimport { Container, Header, Grid, Image } from 'semantic-ui-react';\nimport AgentCardComponent from '../Components/AgentCardComponent';\nimport Cooperate from '../Static/Cooperate.png';\nimport Defect from '../Static/Defect.png';\nimport Copy from '../Static/Copy.png';\nimport Grudge from '../Static/Grudge.png';\nimport Model from '../Static/Model.png';\nimport LSTM from '../Static/LSTM.png';\nimport QLearning from '../Static/QLearning.png';\n\nconst AboutPage = () => {\n    return <Container>\n        <Header as='h2'>Introduction to Prisoner's Dilemma</Header>\n        <div>\n            The prisoner's dilemma is a game commonly analyzed in game theory to show why individuals may not \n            cooperate despite it appearing to be in their best interest to do so.  In this game, two players \n            make a choice to cooperate or defect, and are rewarded based on their action and their opponent's \n            action. The rewards are constrained such that a reward is maximum when one individual defects and \n            the other does not. The next highest reward occurs when both individuals cooperate. Lastly, when both \n            both individuals defect, an individual's reward is minimized. The iterated  prisoner's  dilemma  (IPD)  \n            repeats  this process multiple times, creating a greater incentive to cooperate, motivating coordination \n            from both agents. The goal for this project was to use a combination of deep learning and reinforcement \n            learning to create an agent that could play prisoner's dilemma to a reasonable level of proficiency.\n        </div>\n        <Header as='h2'>Agents for Training</Header>\n        <div>To train our model, we needed it to play against </div>\n        <Container>\n            \n        </Container>\n\n        <Header as='h2'>Custom AI Agent</Header>\n        <div>\n            Our custom model utilized deep learning in the form of a LSTM Neural Network to predict the idenity of \n            the agent it plays against. Then, from the identity of the agent, a custom Q-Table is used to infer the \n            optimal move given the previous history of the game. The LSTM Neural Network \n        </div>\n        <Grid>\n            <Grid.Row centered>\n                <Grid.Column width={12}>\n                    <Image src={Model}/>\n                </Grid.Column>\n            </Grid.Row>\n        </Grid>\n        <Header as='h2'>LSTM for Prediction</Header>\n        <Grid>\n            <Grid.Row centered>\n                <Grid.Column width={10}>\n                    <Image src={LSTM}/>\n                </Grid.Column>\n            </Grid.Row>\n        </Grid>\n        <Header as='h2'>Q-Table for Action</Header>\n        <Grid>\n            <Grid.Row centered>\n                <Grid.Column width={12}>\n                    <Image src={QLearning}/>\n                </Grid.Column>\n            </Grid.Row>\n        </Grid>\n        <Header as='h2'>References</Header>\n    </Container>\n}\n\nexport default AboutPage;"]},"metadata":{},"sourceType":"module"}