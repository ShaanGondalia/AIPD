{"ast":null,"code":"var _jsxFileName = \"/Users/Shufan/Desktop/aipd/frontend/src/Pages/AboutPage.js\";\nimport React from 'react';\nimport { Container, Header, Grid, Image } from 'semantic-ui-react';\nimport AgentCardComponent from '../Components/AgentCardComponent';\nimport Cooperate from '../Static/Cooperate.png';\nimport Defect from '../Static/Defect.png';\nimport Copy from '../Static/Copy.png';\nimport Grudge from '../Static/Grudge.png';\nimport Model from '../Static/Model.png';\nimport LSTM from '../Static/LSTM.png';\nimport QLearning from '../Static/QLearning.png';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\n\nconst AboutPage = () => {\n  return /*#__PURE__*/_jsxDEV(Container, {\n    children: [/*#__PURE__*/_jsxDEV(Header, {\n      as: \"h2\",\n      children: \"Introduction to Prisoner's Dilemma\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 14,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      children: \"The prisoner's dilemma is a game commonly analyzed in game theory to show why individuals may not cooperate despite it appearing to be in their best interest to do so.  In this game, two players make a choice to cooperate or defect, and are rewarded based on their action and their opponent's action. The rewards are constrained such that a reward is maximum when one individual defects and the other does not. The next highest reward occurs when both individuals cooperate. Lastly, when both both individuals defect, an individual's reward is minimized. The iterated  prisoner's  dilemma  (IPD) repeats  this process multiple times, creating a greater incentive to cooperate, motivating coordination from both agents. The goal for this project was to use a combination of deep learning and reinforcement learning to create an agent that could play prisoner's dilemma to a reasonable level of proficiency.\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 15,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Header, {\n      as: \"h2\",\n      children: \"Agents for Training\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 26,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      style: {\n        marginBottom: '2em'\n      },\n      children: \"To train our model, we needed it to play against \"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 27,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Grid, {\n      children: /*#__PURE__*/_jsxDEV(Grid.Row, {\n        centered: true,\n        children: [/*#__PURE__*/_jsxDEV(Grid.Column, {\n          width: 3,\n          children: /*#__PURE__*/_jsxDEV(AgentCardComponent, {\n            image: Cooperate,\n            header: \"Cooperate Agent\",\n            description: \"Agent that cooperates regardless of the opponent moves.\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 31,\n            columnNumber: 21\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 30,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(Grid.Column, {\n          width: 3,\n          children: /*#__PURE__*/_jsxDEV(AgentCardComponent, {\n            image: Defect,\n            header: \"Defect Agent\",\n            description: \"Agent that defects regardless of the opponent moves.\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 38,\n            columnNumber: 21\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 37,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(Grid.Column, {\n          width: 3,\n          children: /*#__PURE__*/_jsxDEV(AgentCardComponent, {\n            image: Copy,\n            header: \"Copy Agent\",\n            description: \"Agent that chooses the previous move of the opponent.\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 45,\n            columnNumber: 21\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 44,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(Grid.Column, {\n          width: 3,\n          children: /*#__PURE__*/_jsxDEV(AgentCardComponent, {\n            image: Grudge,\n            header: \"Grudge Agent\",\n            description: \"Agent that cooperates unless defect on, in which case it always defects.\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 52,\n            columnNumber: 21\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 51,\n          columnNumber: 17\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 29,\n        columnNumber: 13\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 28,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Header, {\n      as: \"h2\",\n      children: \"Custom AI Agent\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 60,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      style: {\n        marginBottom: '2em'\n      },\n      children: \"Our custom model utilized deep learning in the form of a LSTM Neural Network to predict the idenity of the agent it plays against. Then, from the identity of the agent, a custom Q-Table is used to infer the optimal move given the previous history of the game. The input to this model is a set of past moves for both the player and the opponent. The LSTM Neural Network outputs a confidence value for each agent it is trained on which it believes to be the probability that the input move set could be from each agent. The Q-Table table uses the same set of past moves to predict the expected reward given a cooperate decision or a defect decision. Since the LSTM outputs probabilities, one can choose stochastically or deterministically the idenity of the opponent agent, which would affect the choice of Q-Table and may effect the decision of the model. Weighing the optimal moves against the output probability of the LSTM can give a probability that for either choice being the optimal move.\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 61,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Grid, {\n      children: /*#__PURE__*/_jsxDEV(Grid.Row, {\n        centered: true,\n        children: /*#__PURE__*/_jsxDEV(Grid.Column, {\n          width: 12,\n          children: /*#__PURE__*/_jsxDEV(Image, {\n            src: Model\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 76,\n            columnNumber: 21\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 75,\n          columnNumber: 17\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 74,\n        columnNumber: 13\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 73,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Header, {\n      as: \"h2\",\n      children: \"LSTM for Prediction\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 80,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Grid, {\n      children: /*#__PURE__*/_jsxDEV(Grid.Row, {\n        centered: true,\n        children: /*#__PURE__*/_jsxDEV(Grid.Column, {\n          width: 10,\n          children: /*#__PURE__*/_jsxDEV(Image, {\n            src: LSTM\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 84,\n            columnNumber: 21\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 83,\n          columnNumber: 17\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 82,\n        columnNumber: 13\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 81,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Header, {\n      as: \"h2\",\n      children: \"Q-Table for Action\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 88,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      children: \"A Q-Table is mapping of states and actions to a reward. In other words Q:SxA->R The Q-Table was adopted with the objective of maximizing the agent's own reward when playing against an opponent of a known strategy. In order to compute the Q-Table, an agent plays the IPD multiple times against a strategy, populating the rewards associated with the result of performing each action at any state in the game. Since the Q-table is trained to play against one strategy, for a game with N strategies, N Q-tables are trained.\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 89,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Grid, {\n      children: /*#__PURE__*/_jsxDEV(Grid.Row, {\n        centered: true,\n        children: /*#__PURE__*/_jsxDEV(Grid.Column, {\n          width: 12,\n          children: /*#__PURE__*/_jsxDEV(Image, {\n            src: QLearning\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 101,\n            columnNumber: 21\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 100,\n          columnNumber: 17\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 99,\n        columnNumber: 13\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 98,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Header, {\n      as: \"h2\",\n      children: \"References\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 105,\n      columnNumber: 9\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 13,\n    columnNumber: 12\n  }, this);\n};\n\n_c = AboutPage;\nexport default AboutPage;\n\nvar _c;\n\n$RefreshReg$(_c, \"AboutPage\");","map":{"version":3,"sources":["/Users/Shufan/Desktop/aipd/frontend/src/Pages/AboutPage.js"],"names":["React","Container","Header","Grid","Image","AgentCardComponent","Cooperate","Defect","Copy","Grudge","Model","LSTM","QLearning","AboutPage","marginBottom"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,SAASC,SAAT,EAAoBC,MAApB,EAA4BC,IAA5B,EAAkCC,KAAlC,QAA+C,mBAA/C;AACA,OAAOC,kBAAP,MAA+B,kCAA/B;AACA,OAAOC,SAAP,MAAsB,yBAAtB;AACA,OAAOC,MAAP,MAAmB,sBAAnB;AACA,OAAOC,IAAP,MAAiB,oBAAjB;AACA,OAAOC,MAAP,MAAmB,sBAAnB;AACA,OAAOC,KAAP,MAAkB,qBAAlB;AACA,OAAOC,IAAP,MAAiB,oBAAjB;AACA,OAAOC,SAAP,MAAsB,yBAAtB;;;AAEA,MAAMC,SAAS,GAAG,MAAM;AACpB,sBAAO,QAAC,SAAD;AAAA,4BACH,QAAC,MAAD;AAAQ,MAAA,EAAE,EAAC,IAAX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YADG,eAEH;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAFG,eAaH,QAAC,MAAD;AAAQ,MAAA,EAAE,EAAC,IAAX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAbG,eAcH;AAAK,MAAA,KAAK,EAAI;AAACC,QAAAA,YAAY,EAAE;AAAf,OAAd;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAdG,eAeH,QAAC,IAAD;AAAA,6BACI,QAAC,IAAD,CAAM,GAAN;AAAU,QAAA,QAAQ,MAAlB;AAAA,gCACI,QAAC,IAAD,CAAM,MAAN;AAAa,UAAA,KAAK,EAAE,CAApB;AAAA,iCACI,QAAC,kBAAD;AACI,YAAA,KAAK,EAAGR,SADZ;AAEI,YAAA,MAAM,EAAC,iBAFX;AAGI,YAAA,WAAW,EAAC;AAHhB;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,gBADJ,eAQI,QAAC,IAAD,CAAM,MAAN;AAAa,UAAA,KAAK,EAAE,CAApB;AAAA,iCACI,QAAC,kBAAD;AACI,YAAA,KAAK,EAAGC,MADZ;AAEI,YAAA,MAAM,EAAC,cAFX;AAGI,YAAA,WAAW,EAAC;AAHhB;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,gBARJ,eAeI,QAAC,IAAD,CAAM,MAAN;AAAa,UAAA,KAAK,EAAE,CAApB;AAAA,iCACI,QAAC,kBAAD;AACI,YAAA,KAAK,EAAGC,IADZ;AAEI,YAAA,MAAM,EAAC,YAFX;AAGI,YAAA,WAAW,EAAC;AAHhB;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,gBAfJ,eAsBI,QAAC,IAAD,CAAM,MAAN;AAAa,UAAA,KAAK,EAAE,CAApB;AAAA,iCACI,QAAC,kBAAD;AACI,YAAA,KAAK,EAAGC,MADZ;AAEI,YAAA,MAAM,EAAC,cAFX;AAGI,YAAA,WAAW,EAAC;AAHhB;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,gBAtBJ;AAAA;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,YAfG,eA+CH,QAAC,MAAD;AAAQ,MAAA,EAAE,EAAC,IAAX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YA/CG,eAgDH;AAAK,MAAA,KAAK,EAAI;AAACK,QAAAA,YAAY,EAAE;AAAf,OAAd;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAhDG,eA4DH,QAAC,IAAD;AAAA,6BACI,QAAC,IAAD,CAAM,GAAN;AAAU,QAAA,QAAQ,MAAlB;AAAA,+BACI,QAAC,IAAD,CAAM,MAAN;AAAa,UAAA,KAAK,EAAE,EAApB;AAAA,iCACI,QAAC,KAAD;AAAO,YAAA,GAAG,EAAEJ;AAAZ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,YA5DG,eAmEH,QAAC,MAAD;AAAQ,MAAA,EAAE,EAAC,IAAX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAnEG,eAoEH,QAAC,IAAD;AAAA,6BACI,QAAC,IAAD,CAAM,GAAN;AAAU,QAAA,QAAQ,MAAlB;AAAA,+BACI,QAAC,IAAD,CAAM,MAAN;AAAa,UAAA,KAAK,EAAE,EAApB;AAAA,iCACI,QAAC,KAAD;AAAO,YAAA,GAAG,EAAEC;AAAZ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,YApEG,eA2EH,QAAC,MAAD;AAAQ,MAAA,EAAE,EAAC,IAAX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YA3EG,eA4EH;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YA5EG,eAqFH,QAAC,IAAD;AAAA,6BACI,QAAC,IAAD,CAAM,GAAN;AAAU,QAAA,QAAQ,MAAlB;AAAA,+BACI,QAAC,IAAD,CAAM,MAAN;AAAa,UAAA,KAAK,EAAE,EAApB;AAAA,iCACI,QAAC,KAAD;AAAO,YAAA,GAAG,EAAEC;AAAZ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,YArFG,eA4FH,QAAC,MAAD;AAAQ,MAAA,EAAE,EAAC,IAAX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YA5FG;AAAA;AAAA;AAAA;AAAA;AAAA,UAAP;AA8FH,CA/FD;;KAAMC,S;AAiGN,eAAeA,SAAf","sourcesContent":["import React from 'react';\nimport { Container, Header, Grid, Image } from 'semantic-ui-react';\nimport AgentCardComponent from '../Components/AgentCardComponent';\nimport Cooperate from '../Static/Cooperate.png';\nimport Defect from '../Static/Defect.png';\nimport Copy from '../Static/Copy.png';\nimport Grudge from '../Static/Grudge.png';\nimport Model from '../Static/Model.png';\nimport LSTM from '../Static/LSTM.png';\nimport QLearning from '../Static/QLearning.png';\n\nconst AboutPage = () => {\n    return <Container>\n        <Header as='h2'>Introduction to Prisoner's Dilemma</Header>\n        <div>\n            The prisoner's dilemma is a game commonly analyzed in game theory to show why individuals may not \n            cooperate despite it appearing to be in their best interest to do so.  In this game, two players \n            make a choice to cooperate or defect, and are rewarded based on their action and their opponent's \n            action. The rewards are constrained such that a reward is maximum when one individual defects and \n            the other does not. The next highest reward occurs when both individuals cooperate. Lastly, when both \n            both individuals defect, an individual's reward is minimized. The iterated  prisoner's  dilemma  (IPD)  \n            repeats  this process multiple times, creating a greater incentive to cooperate, motivating coordination \n            from both agents. The goal for this project was to use a combination of deep learning and reinforcement \n            learning to create an agent that could play prisoner's dilemma to a reasonable level of proficiency.\n        </div>\n        <Header as='h2'>Agents for Training</Header>\n        <div style = {{marginBottom: '2em'}}>To train our model, we needed it to play against </div>\n        <Grid>\n            <Grid.Row centered>\n                <Grid.Column width={3}>\n                    <AgentCardComponent\n                        image= {Cooperate}\n                        header=\"Cooperate Agent\"\n                        description=\"Agent that cooperates regardless of the opponent moves.\"\n                    />\n                </Grid.Column>\n                <Grid.Column width={3}>\n                    <AgentCardComponent\n                        image= {Defect}\n                        header=\"Defect Agent\"\n                        description=\"Agent that defects regardless of the opponent moves.\"\n                    />\n                </Grid.Column>\n                <Grid.Column width={3}>\n                    <AgentCardComponent\n                        image= {Copy}\n                        header=\"Copy Agent\"\n                        description=\"Agent that chooses the previous move of the opponent.\"\n                    />\n                </Grid.Column>\n                <Grid.Column width={3}>\n                    <AgentCardComponent\n                        image= {Grudge}\n                        header=\"Grudge Agent\"\n                        description=\"Agent that cooperates unless defect on, in which case it always defects.\"\n                    />\n                </Grid.Column>\n            </Grid.Row>\n        </Grid>\n        <Header as='h2'>Custom AI Agent</Header>\n        <div style = {{marginBottom: '2em'}}>\n            Our custom model utilized deep learning in the form of a LSTM Neural Network to predict the idenity of \n            the agent it plays against. Then, from the identity of the agent, a custom Q-Table is used to infer the \n            optimal move given the previous history of the game. The input to this model is a set of past moves for\n            both the player and the opponent. The LSTM Neural Network outputs a confidence value for each agent it is \n            trained on which it believes to be the probability that the input move set could be from each agent. The \n            Q-Table table uses the same set of past moves to predict the expected reward given a cooperate decision or \n            a defect decision. Since the LSTM outputs probabilities, one can choose stochastically or deterministically\n            the idenity of the opponent agent, which would affect the choice of Q-Table and may effect the decision of\n            the model. Weighing the optimal moves against the output probability of the LSTM can give a probability\n            that for either choice being the optimal move.\n        </div>\n        <Grid>\n            <Grid.Row centered>\n                <Grid.Column width={12}>\n                    <Image src={Model}/>\n                </Grid.Column>\n            </Grid.Row>\n        </Grid>\n        <Header as='h2'>LSTM for Prediction</Header>\n        <Grid>\n            <Grid.Row centered>\n                <Grid.Column width={10}>\n                    <Image src={LSTM}/>\n                </Grid.Column>\n            </Grid.Row>\n        </Grid>\n        <Header as='h2'>Q-Table for Action</Header>\n        <div>\n            A Q-Table is mapping of states and actions to a reward. In other words Q:SxA->R\n            The Q-Table was adopted with the objective of maximizing the agent's own reward when \n            playing against an opponent of a known strategy. In order to compute the Q-Table, an \n            agent plays the IPD multiple times against a strategy, populating the rewards associated \n            with the result of performing each action at any state in the game. Since the Q-table is \n            trained to play against one strategy, for a game with N strategies, N Q-tables are trained. \n\n        </div>\n        <Grid>\n            <Grid.Row centered>\n                <Grid.Column width={12}>\n                    <Image src={QLearning}/>\n                </Grid.Column>\n            </Grid.Row>\n        </Grid>\n        <Header as='h2'>References</Header>\n    </Container>\n}\n\nexport default AboutPage;"]},"metadata":{},"sourceType":"module"}